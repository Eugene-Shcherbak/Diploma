{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1daa07a8-faf6-4a9b-8faf-d885cf9dbf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import osmnx as ox\n",
    "import networkx as nx\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9a6cea9-22c5-4c8c-9ef9-8802a5a24719",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ryzen\\AppData\\Local\\Temp\\ipykernel_14492\\618133296.py:1: FutureWarning: The `utils.config` function is deprecated and will be removed in the v2.0.0 release. Instead, use the `settings` module directly to configure a global setting's value. For example, `ox.settings.log_console=True`. See the OSMnx v2 migration guide: https://github.com/gboeing/osmnx/issues/1123\n",
      "  ox.config(log_console=False)\n",
      "C:\\Users\\Ryzen\\AppData\\Local\\Temp\\ipykernel_14492\\618133296.py:3: FutureWarning: The `get_largest_component` function is deprecated and will be removed in the v2.0.0 release. Replace it with `truncate.largest_component` instead. See the OSMnx v2 migration guide: https://github.com/gboeing/osmnx/issues/1123\n",
      "  G = ox.utils_graph.get_largest_component(G, strongly=True)\n"
     ]
    }
   ],
   "source": [
    "ox.config(log_console=False)\n",
    "G = ox.graph_from_place(\"Lviv Oblast, Ukraine\", network_type='drive')\n",
    "G = ox.utils_graph.get_largest_component(G, strongly=True)\n",
    "\n",
    "node_list = list(G.nodes)\n",
    "node_id_map = {n: i for i, n in enumerate(node_list)}\n",
    "node_lookup = {i: n for n, i in node_id_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fff8f020-fd97-4de5-bb99-3b691757c8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim=2, hidden_dim=128):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, node_feats):\n",
    "        return self.net(node_feats).squeeze(-1)\n",
    "\n",
    "def get_node_coordinates(node_id):\n",
    "    node = G.nodes[node_id]\n",
    "    return torch.tensor([node['x'], node['y']], dtype=torch.float32)\n",
    "\n",
    "def get_neighbors(node_id):\n",
    "    return list(G.successors(node_id))\n",
    "\n",
    "def distance(n1, n2):\n",
    "    return ox.distance.euclidean_dist_vec(G.nodes[n1]['y'], G.nodes[n1]['x'],\n",
    "                                          G.nodes[n2]['y'], G.nodes[n2]['x'])\n",
    "\n",
    "def train_policy_network(model_path='policy_model.pt', num_episodes=5000):\n",
    "    policy = PolicyNetwork()\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=1e-3)\n",
    "    gamma = 0.99\n",
    "    print_every = 100\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        start_node = random.choice(node_list)\n",
    "        route = [start_node]\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        \n",
    "        for _ in range(random.randint(4, 10)):\n",
    "            curr = route[-1]\n",
    "            neighbors = get_neighbors(curr)\n",
    "            if not neighbors:\n",
    "                break\n",
    "\n",
    "            feats = torch.stack([get_node_coordinates(n) for n in neighbors])\n",
    "            logits = policy(feats)\n",
    "            probs = torch.softmax(logits, dim=0)\n",
    "\n",
    "            dist = torch.distributions.Categorical(probs)\n",
    "            action = dist.sample()\n",
    "            next_node = neighbors[action.item()]\n",
    "            \n",
    "            route.append(next_node)\n",
    "            log_probs.append(dist.log_prob(action))\n",
    "            rewards.append(-distance(curr, next_node))\n",
    "\n",
    "        returns = []\n",
    "        R = 0\n",
    "        for r in reversed(rewards):\n",
    "            R = r + gamma * R\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.tensor(returns)\n",
    "\n",
    "        loss = 0\n",
    "        for log_prob, R in zip(log_probs, returns):\n",
    "            loss -= log_prob * R\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if episode % print_every == 0:\n",
    "            print(f\"Episode {episode}\")\n",
    "\n",
    "    torch.save(policy.state_dict(), model_path)\n",
    "    return policy\n",
    "\n",
    "def load_policy_network(model_path='policy_model.pt'):\n",
    "    policy = PolicyNetwork()\n",
    "    if os.path.exists(model_path):\n",
    "        policy.load_state_dict(torch.load(model_path))\n",
    "        print(\"Loaded saved model.\")\n",
    "    else:\n",
    "        print(\"Training new model...\")\n",
    "        policy = train_policy_network(model_path)\n",
    "    policy.eval()\n",
    "    return policy\n",
    "\n",
    "def generate_route(policy, lat_lon_points):\n",
    "    path = []\n",
    "    for i in range(len(lat_lon_points) - 1):\n",
    "        start = ox.distance.nearest_nodes(G, lat_lon_points[i][1], lat_lon_points[i][0])\n",
    "        end = ox.distance.nearest_nodes(G, lat_lon_points[i+1][1], lat_lon_points[i+1][0])\n",
    "\n",
    "        current = start\n",
    "        sub_path = [current]\n",
    "        for _ in range(50): #!!!!!!!!!!!!!!!!!!!!!\n",
    "            if current == end:\n",
    "                break\n",
    "            neighbors = get_neighbors(current)\n",
    "            if not neighbors:\n",
    "                break\n",
    "\n",
    "            feats = torch.stack([get_node_coordinates(n) for n in neighbors])\n",
    "            logits = policy(feats)\n",
    "            probs = torch.softmax(logits, dim=0)\n",
    "            action = torch.argmax(probs)\n",
    "            next_node = neighbors[action.item()]\n",
    "            sub_path.append(next_node)\n",
    "            current = next_node\n",
    "\n",
    "        path.extend(sub_path[:-1])\n",
    "    path.append(end)\n",
    "    return path\n",
    "\n",
    "def route_creator(destinations):\n",
    "    return [random.choice(map_points) for _ in range(destinations)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56e50be9-4e6c-4d42-b436-17b689b07130",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_points=[(49.80611220996008, 23.97955612862654),\n",
    "(49.80532285209437, 23.98237781261442),\n",
    "(49.80478275771747, 23.987291619711186),\n",
    "(49.8042495817099, 23.99243073217024),\n",
    "(49.80390336037501, 23.996432588052162),\n",
    "(49.80395183150313, 23.999769256034998),\n",
    "(49.80649303590949, 23.999114796958104),\n",
    "(49.81293216592172, 24.000763128745433),\n",
    "(49.81766865049101, 24.003570346218133),\n",
    "(49.823736810282, 24.00771312858426),\n",
    "(49.82582162024907, 24.00966317414233),\n",
    "(49.832640701340466, 24.01859487887445),\n",
    "(49.83680583050842, 24.02202637853643),\n",
    "(49.840145382636926, 24.027186953530506),\n",
    "(49.84012073274527, 24.028125056144603),\n",
    "(49.839253501750214, 24.03253064738627)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f376b9ba-ad4a-4eba-a831-16ea85fb21a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded saved model.\n"
     ]
    }
   ],
   "source": [
    "policy = load_policy_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "254b4d33-4a23-4db1-ac23-f6b6757c7e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PolicyNetwork(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b741cd7-330a-4cba-937e-dd814a488e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "time to generate route: 0.7916774749755859 s\n",
      "4\n",
      "time to generate route: 1.5070152282714844 s\n",
      "8\n",
      "time to generate route: 3.161550998687744 s\n",
      "16\n",
      "time to generate route: 7.145020008087158 s\n",
      "64\n",
      "time to generate route: 33.2262167930603 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for stops in [2, 4, 8, 16, 64]:\n",
    "    route = route_creator(stops)\n",
    "    print(stops)\n",
    "    \n",
    "    timer_start = time.time()\n",
    "    node_path = generate_route(policy, route)\n",
    "    \n",
    "    timer_end = time.time()\n",
    "    time_total = timer_end - timer_start\n",
    "    print(\"time to generate route: \" + str(time_total) + \" s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5adb083-0a21-4f85-8401-c83fb3dbeaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "route = route_creator(2)\n",
    "node_path = generate_route(policy, route)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
